{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AlmEZohWp6mh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3UvAm9dtB6D",
        "outputId": "9d1761fd-b6f7-4455-da4d-571c2edf75f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d2b367c1e508>:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  test_df = pd.read_csv('drive/MyDrive/Data/test.csv')\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('drive/MyDrive/Data/train.csv')\n",
        "test_df = pd.read_csv('drive/MyDrive/Data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acG6BZia8764",
        "outputId": "bf1f0a28-ab3a-4637-8caf-ce998fd98f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xvlZGGg39e9b"
      },
      "outputs": [],
      "source": [
        "# Fill missing values\n",
        "train_df['question1'] = train_df['question1'].fillna('')\n",
        "train_df['question2'] = train_df['question2'].fillna('')\n",
        "test_df['question1'] = test_df['question1'].fillna('')\n",
        "test_df['question2'] = test_df['question2'].fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CysQ1-GLtR0-"
      },
      "outputs": [],
      "source": [
        "# Preprocessing functions\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "guPINmVdtVtp"
      },
      "outputs": [],
      "source": [
        "# Clean text\n",
        "train_df['question1'] = train_df['question1'].apply(clean_text)\n",
        "train_df['question2'] = train_df['question2'].apply(clean_text)\n",
        "test_df['question1'] = test_df['question1'].apply(clean_text)\n",
        "test_df['question2'] = test_df['question2'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hFxB9xH3tYQN"
      },
      "outputs": [],
      "source": [
        "# TF-IDF vectorization\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "tfidf_q1 = tfidf.fit_transform(train_df['question1'])\n",
        "tfidf_q2 = tfidf.transform(train_df['question2'])\n",
        "\n",
        "tfidf_q1_test = tfidf.transform(test_df['question1'])\n",
        "tfidf_q2_test = tfidf.transform(test_df['question2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "93ddpnsFK8sm"
      },
      "outputs": [],
      "source": [
        "# Function to compute cosine similarity in batches\n",
        "def batch_cosine_similarity(matrix1, matrix2, batch_size=1000):\n",
        "    num_batches = int(np.ceil(matrix1.shape[0] / batch_size))\n",
        "    similarities = np.zeros(matrix1.shape[0])\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i + 1) * batch_size, matrix1.shape[0])\n",
        "        similarities[start:end] = cosine_similarity(matrix1[start:end], matrix2[start:end]).diagonal()\n",
        "    return similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nh4_FbFZtaFE"
      },
      "outputs": [],
      "source": [
        "# Optimized cosine similarity\n",
        "train_cosine_similarities = batch_cosine_similarity(tfidf_q1, tfidf_q2)\n",
        "test_cosine_similarities = batch_cosine_similarity(tfidf_q1_test, tfidf_q2_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FJGrX3pTF2HM"
      },
      "outputs": [],
      "source": [
        "train_df['cosine_similarity'] = train_cosine_similarities\n",
        "test_df['cosine_similarity'] = test_cosine_similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZexPlUJotcTe"
      },
      "outputs": [],
      "source": [
        "# Features and labels\n",
        "X_train = train_df[['cosine_similarity']]\n",
        "y_train = train_df['is_duplicate']\n",
        "X_test = test_df[['cosine_similarity']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d547pEl-te3B"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ayP3a0SDtgmb"
      },
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1ZuGP41tiTN",
        "outputId": "c1dfc9a1-f426-408f-d1cb-fb71b60dccab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.6486433006010537\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.80      0.74     50803\n",
            "           1       0.54      0.40      0.46     30055\n",
            "\n",
            "    accuracy                           0.65     80858\n",
            "   macro avg       0.61      0.60      0.60     80858\n",
            "weighted avg       0.63      0.65      0.64     80858\n",
            "\n",
            "Random Forest Accuracy: 0.6505355066907418\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.69      0.71     50803\n",
            "           1       0.53      0.59      0.56     30055\n",
            "\n",
            "    accuracy                           0.65     80858\n",
            "   macro avg       0.63      0.64      0.63     80858\n",
            "weighted avg       0.66      0.65      0.65     80858\n",
            "\n",
            "XGBoost Accuracy: 0.6491874644438398\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.81      0.74     50803\n",
            "           1       0.54      0.38      0.44     30055\n",
            "\n",
            "    accuracy                           0.65     80858\n",
            "   macro avg       0.61      0.59      0.59     80858\n",
            "weighted avg       0.63      0.65      0.63     80858\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_split, y_train_split)\n",
        "    y_pred = model.predict(X_val_split)\n",
        "    print(f'{name} Accuracy: {accuracy_score(y_val_split, y_pred)}')\n",
        "    print(classification_report(y_val_split, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}